{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "pytorch_linear_regression.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN4zsoeFNMAF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Linear Regression with PyTorch\n",
        "## 1. About Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JC-ijthNMAJ",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Simple Linear Regression Basics\n",
        "- Allows us to understand **relationship** between two **continuous variables**\n",
        "- Example\n",
        "    - x: independent variable\n",
        "        - weight\n",
        "    - y: dependent variable\n",
        "        - height\n",
        "- $y = \\alpha x + \\beta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbj4ta7HNMAK",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Example of simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyt79iuWNMAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "528f1158-5bad-4e7e-f3f3-57a013ec454a"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)\n",
        "n = 50\n",
        "x = np.random.randn(n)\n",
        "y = x * np.random.randn(n)\n",
        "\n",
        "colors = np.random.rand(n)\n",
        "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))\n",
        "\n",
        "plt.scatter(x, y, c=colors, alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBANToJcCCEa3P8PMoFRIOQWiiUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dmTztAUNMAT",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Aim of Linear Regression\n",
        "- Minimize the distance between the points and the line ($y = \\alpha x + \\beta$)\n",
        "- Adjusting\n",
        "    - Coefficient: $\\alpha$\n",
        "    - Bias/intercept: $\\beta$\n",
        "\n",
        "## 2. Building a Linear Regression Model with PyTorch\n",
        "\n",
        "### 2.1 Example\n",
        "- Coefficient: $\\alpha = 2$\n",
        "- Bias/intercept: $\\beta = 1$\n",
        "- Equation: $y = 2x + 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfzGdrIGNMAU",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Building a Toy Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf_emJSVNMAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0fe6f08-9d6d-4be7-bcd1-6c9995c99eb8"
      },
      "source": [
        "x_values = [i for i in range(11)]\n",
        "x_values"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A3Hr1o7NMAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1030c05-c722-43ae-83f2-6aee564b6e0c"
      },
      "source": [
        "# Convert to numpy\n",
        "x_train = np.array(x_values, dtype=np.float32)\n",
        "x_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPguWBShNMAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58fa8577-6c20-4087-8c1d-437b4e2b992f"
      },
      "source": [
        "# IMPORTANT: 2D required\n",
        "x_train = x_train.reshape(-1, 1)\n",
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQYhWNayNMAl",
        "colab_type": "text"
      },
      "source": [
        "$y = 2x + 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2ItFzRcNMAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd9d4a4a-1268-4978-d674-47e98c8827ea"
      },
      "source": [
        "y_values = [2*i + 1 for i in x_values]\n",
        "y_values"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RGdTk3qNMA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff23947f-17c7-4b3c-9cf8-050904f068a1"
      },
      "source": [
        "y_train = np.array(y_values, dtype=np.float32)\n",
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTcK31BxNMA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0c084df-6c08-416f-f26e-2ee5605a964a"
      },
      "source": [
        "# IMPORTANT: 2D required\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv3DzHSPNMA-",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbRO-KY_NMA_",
        "colab_type": "text"
      },
      "source": [
        "**Critical Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnH0rKSNMBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMmllVG3NMBE",
        "colab_type": "text"
      },
      "source": [
        "**Create Model**\n",
        "1. Linear model\n",
        "    - True Equation: $y = 2x + 1$\n",
        "2. Forward\n",
        "    - Example\n",
        "        - Input $x = 1 $\n",
        "        - Output $\\hat y = ?$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNMx69XQNMBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create class\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgXHO3aVNMBL",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate Model Class**\n",
        "- input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "- desired output: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCWjRgB2NMBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 1\n",
        "output_dim = 1\n",
        "\n",
        "model = LinearRegressionModel(input_dim, output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ETO48zUNMBQ",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate Loss Class**\n",
        "- MSE Loss: Mean Squared Error\n",
        "- $MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat y_i - y_i)$\n",
        "    - $\\hat y$: prediction\n",
        "    - $y$: true value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glXMiw1SNMBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SP3RFxONMBU",
        "colab_type": "text"
      },
      "source": [
        "**Instantiate Optimizer Class**\n",
        "- Simplified equation\n",
        "    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n",
        "        - $\\theta$: parameters (our variables)\n",
        "        - $\\eta$: learning rate (how fast we want to learn)\n",
        "        - $\\nabla_\\theta$: parameters' gradients\n",
        "- Even simplier equation\n",
        "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "        - parameters: $\\alpha$ and $\\beta$ in $ y = \\alpha x + \\beta$\n",
        "        - desired parameters: $\\alpha = 2$ and $\\beta = 1$ in $ y = 2x + 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFEkaHdENMBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yO_qx4ENMBZ",
        "colab_type": "text"
      },
      "source": [
        "**Train Model**\n",
        "- 1 epoch: going through the whole x_train data once\n",
        "    - 100 epochs: \n",
        "        - 100x mapping `x_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]`\n",
        "        \n",
        "- Process \n",
        "    1. Convert inputs/labels to tensors with gradients\n",
        "    2. Clear gradient buffets\n",
        "    3. Get output given inputs \n",
        "    4. Get loss\n",
        "    5. Get gradients w.r.t. parameters\n",
        "    6. Update parameters using gradients\n",
        "        - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "    7. REPEAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLQeKucONMBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AMEuGMa5NMBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1e7bc65-3973-456b-db8d-c4e381b7d544"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch += 1\n",
        "    # Convert numpy array to torch Variable\n",
        "    inputs = torch.from_numpy(x_train).requires_grad_()\n",
        "    labels = torch.from_numpy(y_train)\n",
        "\n",
        "    # Clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    # Forward to get output\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Calculate Loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Updating parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 111.84110260009766\n",
            "epoch 2, loss 9.128509521484375\n",
            "epoch 3, loss 0.7504992485046387\n",
            "epoch 4, loss 0.06706570088863373\n",
            "epoch 5, loss 0.01125488430261612\n",
            "epoch 6, loss 0.006637908052653074\n",
            "epoch 7, loss 0.006197448819875717\n",
            "epoch 8, loss 0.006098343525081873\n",
            "epoch 9, loss 0.006027775350958109\n",
            "epoch 10, loss 0.005960265174508095\n",
            "epoch 11, loss 0.005893712863326073\n",
            "epoch 12, loss 0.005827887449413538\n",
            "epoch 13, loss 0.005762794520705938\n",
            "epoch 14, loss 0.005698444787412882\n",
            "epoch 15, loss 0.00563481030985713\n",
            "epoch 16, loss 0.0055719041265547276\n",
            "epoch 17, loss 0.005509680602699518\n",
            "epoch 18, loss 0.0054481602273881435\n",
            "epoch 19, loss 0.00538730900734663\n",
            "epoch 20, loss 0.005327157210558653\n",
            "epoch 21, loss 0.005267664324492216\n",
            "epoch 22, loss 0.0052088117226958275\n",
            "epoch 23, loss 0.0051506757736206055\n",
            "epoch 24, loss 0.005093161948025227\n",
            "epoch 25, loss 0.005036287009716034\n",
            "epoch 26, loss 0.004979999270290136\n",
            "epoch 27, loss 0.00492442911490798\n",
            "epoch 28, loss 0.00486941821873188\n",
            "epoch 29, loss 0.004815090913325548\n",
            "epoch 30, loss 0.004761321470141411\n",
            "epoch 31, loss 0.004708117339760065\n",
            "epoch 32, loss 0.004655530676245689\n",
            "epoch 33, loss 0.0046035489067435265\n",
            "epoch 34, loss 0.004552181344479322\n",
            "epoch 35, loss 0.004501312971115112\n",
            "epoch 36, loss 0.004451073240488768\n",
            "epoch 37, loss 0.004401332698762417\n",
            "epoch 38, loss 0.004352203104645014\n",
            "epoch 39, loss 0.0043035936541855335\n",
            "epoch 40, loss 0.004255552310496569\n",
            "epoch 41, loss 0.004208007361739874\n",
            "epoch 42, loss 0.004161038435995579\n",
            "epoch 43, loss 0.004114553797990084\n",
            "epoch 44, loss 0.004068625159561634\n",
            "epoch 45, loss 0.0040231808088719845\n",
            "epoch 46, loss 0.003978246822953224\n",
            "epoch 47, loss 0.003933847416192293\n",
            "epoch 48, loss 0.0038899078499525785\n",
            "epoch 49, loss 0.003846497507765889\n",
            "epoch 50, loss 0.003803509520366788\n",
            "epoch 51, loss 0.003761048661544919\n",
            "epoch 52, loss 0.0037190625444054604\n",
            "epoch 53, loss 0.003677481785416603\n",
            "epoch 54, loss 0.003636429086327553\n",
            "epoch 55, loss 0.0035958532243967056\n",
            "epoch 56, loss 0.0035556734073907137\n",
            "epoch 57, loss 0.003515956224873662\n",
            "epoch 58, loss 0.003476708196103573\n",
            "epoch 59, loss 0.0034379027783870697\n",
            "epoch 60, loss 0.003399503882974386\n",
            "epoch 61, loss 0.0033615475986152887\n",
            "epoch 62, loss 0.003323987126350403\n",
            "epoch 63, loss 0.0032868837006390095\n",
            "epoch 64, loss 0.0032501679379493\n",
            "epoch 65, loss 0.003213874762877822\n",
            "epoch 66, loss 0.003177979961037636\n",
            "epoch 67, loss 0.0031425112392753363\n",
            "epoch 68, loss 0.0031074238941073418\n",
            "epoch 69, loss 0.003072718158364296\n",
            "epoch 70, loss 0.0030384010169655085\n",
            "epoch 71, loss 0.0030044815503060818\n",
            "epoch 72, loss 0.0029709218069911003\n",
            "epoch 73, loss 0.0029377478640526533\n",
            "epoch 74, loss 0.0029049369040876627\n",
            "epoch 75, loss 0.0028725212905555964\n",
            "epoch 76, loss 0.0028404344338923693\n",
            "epoch 77, loss 0.002808723133057356\n",
            "epoch 78, loss 0.002777354558929801\n",
            "epoch 79, loss 0.0027463326696306467\n",
            "epoch 80, loss 0.0027156479191035032\n",
            "epoch 81, loss 0.0026853259187191725\n",
            "epoch 82, loss 0.002655354095622897\n",
            "epoch 83, loss 0.0026257100980728865\n",
            "epoch 84, loss 0.002596386708319187\n",
            "epoch 85, loss 0.0025673815980553627\n",
            "epoch 86, loss 0.0025387124624103308\n",
            "epoch 87, loss 0.002510365331545472\n",
            "epoch 88, loss 0.002482330659404397\n",
            "epoch 89, loss 0.002454612636938691\n",
            "epoch 90, loss 0.0024272212758660316\n",
            "epoch 91, loss 0.0024001088459044695\n",
            "epoch 92, loss 0.002373301424086094\n",
            "epoch 93, loss 0.002346797613427043\n",
            "epoch 94, loss 0.0023206055629998446\n",
            "epoch 95, loss 0.0022946782410144806\n",
            "epoch 96, loss 0.0022690431214869022\n",
            "epoch 97, loss 0.0022437009029090405\n",
            "epoch 98, loss 0.0022186627611517906\n",
            "epoch 99, loss 0.00219389284029603\n",
            "epoch 100, loss 0.0021693732123821974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn-5Ld25NMBo",
        "colab_type": "text"
      },
      "source": [
        "**Compare Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUbEQM2bNMBp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2a527ea4-19f6-4205-c7e8-a0726efccde8"
      },
      "source": [
        "# Purely inference\n",
        "predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()\n",
        "predicted"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.9133581],\n",
              "       [ 2.9258351],\n",
              "       [ 4.9383125],\n",
              "       [ 6.95079  ],\n",
              "       [ 8.963266 ],\n",
              "       [10.975743 ],\n",
              "       [12.988221 ],\n",
              "       [15.000698 ],\n",
              "       [17.013176 ],\n",
              "       [19.025654 ],\n",
              "       [21.03813  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY25ffqtNMBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd0989ed-0e82-41c8-d071-bc349f3ced99"
      },
      "source": [
        "# y = 2x + 1 \n",
        "y_train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.],\n",
              "       [ 3.],\n",
              "       [ 5.],\n",
              "       [ 7.],\n",
              "       [ 9.],\n",
              "       [11.],\n",
              "       [13.],\n",
              "       [15.],\n",
              "       [17.],\n",
              "       [19.],\n",
              "       [21.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZQClA8JNMBv",
        "colab_type": "text"
      },
      "source": [
        "**Plot Graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuHmV7ZVNMBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "3fdd9908-81c0-4b02-bce4-1868a3185daf"
      },
      "source": [
        "# Clear figure\n",
        "plt.clf()\n",
        "\n",
        "# Get predictions\n",
        "predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()\n",
        "\n",
        "# Plot true data\n",
        "plt.plot(x_train, y_train, 'go', label='True data', alpha=0.5)\n",
        "\n",
        "# Plot predictions\n",
        "plt.plot(x_train, predicted, '--', label='Predictions', alpha=0.5)\n",
        "\n",
        "# Legend and plot\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEOTaKQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlaNyJpUNMBz",
        "colab_type": "text"
      },
      "source": [
        "**Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSaTJebeNMB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model = False\n",
        "if save_model is True:\n",
        "    # Saves only parameters\n",
        "    # alpha & beta\n",
        "    torch.save(model.state_dict(), 'awesome_model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzAo5_b3NMB4",
        "colab_type": "text"
      },
      "source": [
        "**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHJOCnTMNMB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_model = False\n",
        "if load_model is True:\n",
        "    model.load_state_dict(torch.load('awesome_model.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vOg4WXINMB9",
        "colab_type": "text"
      },
      "source": [
        "## 3. Building a Linear Regression Model with PyTorch (GPU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iQKDcgZNMB-",
        "colab_type": "text"
      },
      "source": [
        "**CPU Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAJgqkavNMB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "STEP 1: CREATE MODEL CLASS\n",
        "'''\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "'''\n",
        "STEP 2: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "\n",
        "model = LinearRegressionModel(input_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 3: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "'''\n",
        "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 5: TRAIN THE MODEL\n",
        "'''\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    epoch += 1\n",
        "    # Convert numpy array to torch Variable\n",
        "    inputs = torch.from_numpy(x_train).requires_grad_()\n",
        "    labels = torch.from_numpy(y_train)\n",
        "\n",
        "    # Clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    # Forward to get output\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Calculate Loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Updating parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvJ_Bw-sNMCD",
        "colab_type": "text"
      },
      "source": [
        "GPU: 2 things must be on GPU\n",
        "- `model`\n",
        "- `tensors with gradients`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVrYmmbbNMCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "992f64fd-83a6-4b58-d438-0c98a72e684d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "STEP 1: CREATE MODEL CLASS\n",
        "'''\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "'''\n",
        "STEP 2: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "\n",
        "model = LinearRegressionModel(input_dim, output_dim)\n",
        "\n",
        "\n",
        "#######################\n",
        "#  USE GPU FOR MODEL  #\n",
        "#######################\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # check if we have cpu or gpu\n",
        "model.to(device)\n",
        "\n",
        "'''\n",
        "STEP 3: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "'''\n",
        "STEP 4: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 5: TRAIN THE MODEL\n",
        "'''\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    epoch += 1\n",
        "    \n",
        "    #######################\n",
        "    #  USE GPU FOR MODEL  #\n",
        "    #######################\n",
        "    inputs = torch.from_numpy(x_train).to(device)\n",
        "    labels = torch.from_numpy(y_train).to(device)\n",
        "    \n",
        "    # Clear gradients w.r.t. parameters\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    # Forward to get output\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Calculate Loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # Updating parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Logging\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 264.71337890625\n",
            "epoch 2, loss 21.59242820739746\n",
            "epoch 3, loss 1.7618062496185303\n",
            "epoch 4, loss 0.1442802995443344\n",
            "epoch 5, loss 0.012337516993284225\n",
            "epoch 6, loss 0.0015689523424953222\n",
            "epoch 7, loss 0.0006843228475190699\n",
            "epoch 8, loss 0.000605940236710012\n",
            "epoch 9, loss 0.0005934059736318886\n",
            "epoch 10, loss 0.0005863124970346689\n",
            "epoch 11, loss 0.0005797212943434715\n",
            "epoch 12, loss 0.0005732451681979001\n",
            "epoch 13, loss 0.0005668388912454247\n",
            "epoch 14, loss 0.0005605165497399867\n",
            "epoch 15, loss 0.0005542627186514437\n",
            "epoch 16, loss 0.0005480669206008315\n",
            "epoch 17, loss 0.0005419516819529235\n",
            "epoch 18, loss 0.0005359028000384569\n",
            "epoch 19, loss 0.0005299148615449667\n",
            "epoch 20, loss 0.0005239908932708204\n",
            "epoch 21, loss 0.0005181479500606656\n",
            "epoch 22, loss 0.0005123556475155056\n",
            "epoch 23, loss 0.0005066324374638498\n",
            "epoch 24, loss 0.0005009775632061064\n",
            "epoch 25, loss 0.0004953844472765923\n",
            "epoch 26, loss 0.0004898490733467042\n",
            "epoch 27, loss 0.0004843841015826911\n",
            "epoch 28, loss 0.0004789672966580838\n",
            "epoch 29, loss 0.0004736288683488965\n",
            "epoch 30, loss 0.00046832935186102986\n",
            "epoch 31, loss 0.0004631023621186614\n",
            "epoch 32, loss 0.00045792918535880744\n",
            "epoch 33, loss 0.00045282766222953796\n",
            "epoch 34, loss 0.00044776106369681656\n",
            "epoch 35, loss 0.0004427614330779761\n",
            "epoch 36, loss 0.000437812355812639\n",
            "epoch 37, loss 0.00043293103226460516\n",
            "epoch 38, loss 0.00042808838770724833\n",
            "epoch 39, loss 0.00042330636642873287\n",
            "epoch 40, loss 0.000418583833379671\n",
            "epoch 41, loss 0.00041390443220734596\n",
            "epoch 42, loss 0.0004092860617674887\n",
            "epoch 43, loss 0.0004047141701448709\n",
            "epoch 44, loss 0.00040019903099164367\n",
            "epoch 45, loss 0.0003957296721637249\n",
            "epoch 46, loss 0.0003913056571036577\n",
            "epoch 47, loss 0.0003869319334626198\n",
            "epoch 48, loss 0.00038262121961452067\n",
            "epoch 49, loss 0.00037833844544366\n",
            "epoch 50, loss 0.0003741254040505737\n",
            "epoch 51, loss 0.00036994245601817966\n",
            "epoch 52, loss 0.0003658169589471072\n",
            "epoch 53, loss 0.00036173086846247315\n",
            "epoch 54, loss 0.00035768552334047854\n",
            "epoch 55, loss 0.0003536896256264299\n",
            "epoch 56, loss 0.0003497443103697151\n",
            "epoch 57, loss 0.00034583848901093006\n",
            "epoch 58, loss 0.00034197894274257123\n",
            "epoch 59, loss 0.0003381529822945595\n",
            "epoch 60, loss 0.0003343852295074612\n",
            "epoch 61, loss 0.0003306417493149638\n",
            "epoch 62, loss 0.0003269558947067708\n",
            "epoch 63, loss 0.00032329984242096543\n",
            "epoch 64, loss 0.00031969184055924416\n",
            "epoch 65, loss 0.00031611978192813694\n",
            "epoch 66, loss 0.000312596297590062\n",
            "epoch 67, loss 0.0003091043618042022\n",
            "epoch 68, loss 0.0003056430723518133\n",
            "epoch 69, loss 0.0003022327146027237\n",
            "epoch 70, loss 0.000298860773909837\n",
            "epoch 71, loss 0.00029552262276411057\n",
            "epoch 72, loss 0.00029222367447800934\n",
            "epoch 73, loss 0.00028895633295178413\n",
            "epoch 74, loss 0.0002857277577277273\n",
            "epoch 75, loss 0.00028253960772417486\n",
            "epoch 76, loss 0.0002793845487758517\n",
            "epoch 77, loss 0.0002762613003142178\n",
            "epoch 78, loss 0.0002731842396315187\n",
            "epoch 79, loss 0.0002701283083297312\n",
            "epoch 80, loss 0.0002671212423592806\n",
            "epoch 81, loss 0.0002641294850036502\n",
            "epoch 82, loss 0.00026118746609427035\n",
            "epoch 83, loss 0.00025826183264143765\n",
            "epoch 84, loss 0.00025538518093526363\n",
            "epoch 85, loss 0.0002525276504456997\n",
            "epoch 86, loss 0.0002497122040949762\n",
            "epoch 87, loss 0.00024692085571587086\n",
            "epoch 88, loss 0.00024417194072157145\n",
            "epoch 89, loss 0.00024143372138496488\n",
            "epoch 90, loss 0.00023874655016697943\n",
            "epoch 91, loss 0.00023607874754816294\n",
            "epoch 92, loss 0.0002334352320758626\n",
            "epoch 93, loss 0.00023083447013050318\n",
            "epoch 94, loss 0.00022825281484983861\n",
            "epoch 95, loss 0.00022570240253116935\n",
            "epoch 96, loss 0.0002231885737273842\n",
            "epoch 97, loss 0.000220695132156834\n",
            "epoch 98, loss 0.00021822642884217203\n",
            "epoch 99, loss 0.0002157931012334302\n",
            "epoch 100, loss 0.00021338387159630656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xi3cEZ1NMCI",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd-3_Q1hNMCK",
        "colab_type": "text"
      },
      "source": [
        "- Simple **linear regression basics**\n",
        "    - $y = Ax + B$\n",
        "    - $y = 2x + 1$\n",
        "- **Example** of simple linear regression\n",
        "- **Aim** of linear regression\n",
        "    - Minimizing distance between the points and the line\n",
        "        - Calculate \"distance\" through `MSE`\n",
        "        - Calculate `gradients`\n",
        "        - Update parameters with `parameters = parameters - learning_rate * gradients`\n",
        "        - Slowly update parameters $A$ and $B$ model the linear relationship between $y$ and $x$ of the form $y = 2x + 1$\n",
        "- Built a linear regression **model** in **CPU and GPU**\n",
        "    - Step 1: Create Model Class\n",
        "    - Step 2: Instantiate Model Class\n",
        "    - Step 3: Instantiate Loss Class\n",
        "    - Step 4: Instantiate Optimizer Class\n",
        "    - Step 5: Train Model\n",
        "- Important things to be on **GPU**\n",
        "    - `model`\n",
        "    - `tensors with gradients`\n",
        "- How to bring to **GPU**?\n",
        "    - `model_name.cuda()`\n",
        "    - `variable_name.cuda()`"
      ]
    }
  ]
}